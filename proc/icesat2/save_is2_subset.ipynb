{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import h5py\n",
    "from pyproj import Transformer, CRS\n",
    "from shapely.geometry import asPolygon, MultiPolygon, Point\n",
    "import geopandas as gpd\n",
    "from astropy.time import Time\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gps2dyr(time):\n",
    "    \"\"\" Converte GPS time to decimal years. \"\"\"\n",
    "    return Time(time, format='gps').decimalyear\n",
    "\n",
    "\n",
    "def read_reduced_atl06(file, xlims, ylims):\n",
    "    ll2xy = Transformer.from_crs(4326,3031, always_xy = True)\n",
    "    group = ['/gt1l', '/gt1r', '/gt2l', '/gt2r', '/gt3l', '/gt3r']\n",
    "    dfs = []\n",
    "    for k,g in enumerate(group):\n",
    "        e = g in h5py.File(file,'r')\n",
    "        if not e:\n",
    "            continue\n",
    "        with h5py.File(file,'r') as fi:\n",
    "            lat = fi[g+'/land_ice_segments/latitude'][:]\n",
    "            lon = fi[g+'/land_ice_segments/longitude'][:]\n",
    "            h_li = fi[g+'/land_ice_segments/h_li'][:]\n",
    "            quality = fi[g+'/land_ice_segments/atl06_quality_summary'][:]\n",
    "            rgt = fi['/orbit_info/rgt'][:] * np.ones(len(lat))\n",
    "            t_ref = fi['/ancillary_data/atlas_sdp_gps_epoch'][:]\n",
    "            t_dt = fi[g+'/land_ice_segments/delta_time'][:]\n",
    "            group = []\n",
    "        x,y = ll2xy.transform(lon,lat)\n",
    "        t_gps = t_ref + t_dt\n",
    "        t_year = gps2dyr(t_gps)\n",
    "        \n",
    "        idx = (x>=xlims[0]) & (x <= xlims[1]) & (y>=ylims[0]) & (y <= ylims[1]) & (quality==0)\n",
    "        if sum(idx) > 0:\n",
    "            dfs.append(pd.DataFrame(data = {'x': x[idx], \n",
    "                                            'y': y[idx], \n",
    "                                            'h': h_li[idx], \n",
    "                                            't_fracyr': t_year[idx], \n",
    "                                            'lon': lon[idx], \n",
    "                                            'lat': lat[idx], \n",
    "                                            'quality': quality[idx]}))\n",
    "    if len(dfs) > 0:\n",
    "        df = pd.concat(dfs)    \n",
    "        return df\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_fold = 'data/is2/filelists' #made by antarctic trackmap \n",
    "lakes = pd.DataFrame(data = {'name': ['slm'], \n",
    "                             'fullname': ['MercerSubglacialLake']})\n",
    "# you really should pull the outline file fresh from github (see figure 1 plotting code)\n",
    "outline_file = '/Users/siegfried/Documents/data/lakeoutlines/SiegfriedFricker2018/SiegfriedFricker2018-outlines.h5'\n",
    "\n",
    "# import subglacial lake outlines (Siegfried & Fricker, 2018)\n",
    "h5f = h5py.File(outline_file, 'r')\n",
    "outline_geometries = [] # store polygons\n",
    "citations = [] # store citation information\n",
    "\n",
    "# we're going to calculate geodesic lake area because that is often screwed up \n",
    "# and occasionally incorrect in the literature\n",
    "areas = []\n",
    "\n",
    "# we're going to need to do some coordinate transforms for the geodesic area\n",
    "# define CRS for Antarcica and make a converter from xy to ll\n",
    "crs_ll = \"EPSG:4326\" # wgs84 in lon,lat \n",
    "crs_xy = h5f.attrs.get('proj_crs') # get projection from hdf5 file\n",
    "xy_to_ll = Transformer.from_crs(crs_xy, crs_ll, always_xy = True) # make coord transformer\n",
    "geod = CRS(crs_ll).get_geod() # geod object for calculating area on defined ellipsoid\n",
    "\n",
    "# look through each lake and load all of it's info\n",
    "for lake in h5f.keys():\n",
    "    outline_x = h5f[lake]['x'][:]\n",
    "    outline_y = h5f[lake]['y'][:]\n",
    "    outlines_xy = np.stack((outline_x, outline_y),axis=2).reshape(outline_x.shape[1], 2)\n",
    "    \n",
    "    # A single lake with multiple polygons is NaN broken---need to identify and\n",
    "    # load as a MultiPolygon. Otherwise it's easy (just load as polygon)\n",
    "    if np.isnan(outlines_xy)[:,0].sum() == 0:\n",
    "        geometry = asPolygon(outlines_xy)\n",
    "        lon, lat = xy_to_ll.transform(outlines_xy[:,0], outlines_xy[:,1])\n",
    "        this_area = abs(geod.polygon_area_perimeter(lon,lat)[0])/1e6\n",
    "    else:\n",
    "        this_area = 0\n",
    "        # break at NaN values and load each as separate polygons\n",
    "        idx = np.where(np.isnan(outlines_xy[:,0]))[0]\n",
    "        \n",
    "        # grab outline of first lake before getting into the loop\n",
    "        this_outline = outlines_xy[0:idx[0],:]\n",
    "        pgons = [asPolygon(this_outline)] # put the first polygon in a list\n",
    "        lon,lat = xy_to_ll.transform(this_outline[:,0], this_outline[:,1]) \n",
    "        this_area += abs(geod.polygon_area_perimeter(lon,lat)[0])/1e6 # add its area\n",
    "        for i in np.arange(0,len(idx)):\n",
    "            if i == len(idx)-1:\n",
    "                this_outline = outlines_xy[idx[i]+1:,:]\n",
    "            else:\n",
    "                this_outline = outlines_xy[idx[i]+1:idx[i+1]]\n",
    "                \n",
    "            pgons.append(asPolygon(this_outline))\n",
    "            lon,lat = xy_to_ll.transform(this_outline[:,0], this_outline[:,1])\n",
    "            this_area += abs(geod.polygon_area_perimeter(lon,lat)[0])/1e6\n",
    "        geometry = MultiPolygon(pgons)\n",
    "        \n",
    "    # append all the results in the right place\n",
    "    outline_geometries.append(geometry)\n",
    "    citations.append(h5f[lake].attrs.get('citation')[0].decode('UTF-8'))\n",
    "    areas.append(this_area)\n",
    "\n",
    "# make a pandas dataframe with all the necessary info\n",
    "df = pd.DataFrame(zip(h5f.keys(), outline_geometries, areas, citations), \n",
    "                  columns=['name', 'geometry', 'area (km^2)', 'cite'])\n",
    "gdf = gpd.GeoDataFrame(df, crs=crs_xy, geometry=outline_geometries)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,l in enumerate(lakes['name']):\n",
    "    start_time = time.time()\n",
    "    fullname = lakes['fullname'].iloc[i]\n",
    "    if fullname != 'ConwaySubglacialLake':\n",
    "        print(fullname)\n",
    "        filelist = filelist_fold + '/' + l + \"_filelist.txt\"\n",
    "        files = pd.read_csv(filelist, header = None, names=['filename'])\n",
    "        outline = gdf[gdf['name'] == fullname]\n",
    "        region = outline['geometry'].buffer(10000)\n",
    "\n",
    "        bbox = region.bounds\n",
    "        dfs=[]\n",
    "        for idx,row in files.iterrows():\n",
    "            if np.mod(idx,100)==0:\n",
    "                print(\"...\" + str(idx) + ' of ' + str(len(files)))\n",
    "            df = read_reduced_atl06(row['filename'], \n",
    "                                    [bbox['minx'].iloc[0], bbox['maxx'].iloc[0]],\n",
    "                                    [bbox['miny'].iloc[0], bbox['maxy'].iloc[0]])\n",
    "            if len(df) > 0:\n",
    "                #print(row['filename'],len(df))\n",
    "                pts = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y))\n",
    "                pts = pts.set_crs(CRS(\"EPSG:3031\"))\n",
    "                gdf_reg = gpd.GeoDataFrame(geometry = region)\n",
    "                gdf_reg.reset_index(drop = True, inplace = True)\n",
    "\n",
    "                inpoly=pts.within(gdf_reg.at[0,'geometry'])\n",
    "                dfs.append(df.loc[inpoly])\n",
    "        alldata = pd.concat(dfs)\n",
    "        alldata.to_pickle('data/is2/' + fullname + '.is2.atl06.004.pkl')\n",
    "        print(\"--- %s hours ---\" % ((time.time() - start_time)/(60*60)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siegvent2023",
   "language": "python",
   "name": "siegvent2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
