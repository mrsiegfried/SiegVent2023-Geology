{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2758ed",
   "metadata": {},
   "source": [
    "Code to process data for and plot Figure 1 from Siegfried\\*, Venturelli\\*, et al. (2023)\n",
    "\n",
    "This notebook also calculates the scaling ratio between the long-term GPS station (LA09)<br>\n",
    "and the drill site (LA12/LA17), then estimates the height-change at the drill site between<br>\n",
    "low stand and high stand in order to compare surface observations (GPS) to lake water column thickness.\n",
    "\n",
    "Email siegfried@mines.edu and venturelli@mines.edu with any questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5622ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygmt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS, Transformer\n",
    "from shapely.geometry import Polygon, MultiPolygon, LineString, MultiLineString\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import xarray as xr\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean\n",
    "import scipy.io as io\n",
    "import datetime\n",
    "\n",
    "quickplot = False # boolean to speed up plotting when developing\n",
    "out='siegvent2023-fig1.pdf' # Name the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's hard code a couple important tidbits about drilling\n",
    "\n",
    "######\n",
    "# Quick geodetic to PS71 converter from pyproj\n",
    "def ll2xy(lon, lat):\n",
    "    \"\"\"\n",
    "    Transform coordinates from geodetic coordinates (lon, lat)\n",
    "    to Antarctic Polar Stereograph coordinates (x, y)\n",
    "    \n",
    "    x, y = ps2ll(lon, lat)\n",
    "    \"\"\"\n",
    "    crs_ll = CRS(\"EPSG:4326\")\n",
    "    crs_xy = CRS(\"EPSG:3031\")\n",
    "    ll_to_xy = Transformer.from_crs(crs_ll, crs_xy, always_xy = True)\n",
    "    x, y = ll_to_xy.transform(lon, lat)\n",
    "    return x, y\n",
    "######\n",
    "\n",
    "# Hard code borehole coordinate @ breakthrough, calculated by GPS survey of borehole on 17 Jan, \n",
    "# then advected the borehole coordinate based on the change in position of GPS LA17 over the \n",
    "# time period between the borehole survey and break through\n",
    "drill_coord = [-149.50133981653366,-84.6402871908142] # Based on 1 hr survey centered on 17 Jan 2019, 23:30\n",
    "drillx, drilly = ll2xy(drill_coord[0], drill_coord[1])\n",
    "breakthrough = 2018 + (360 + (21+34/60)/24 + 13/24 - 1)/365  #26 Dec 2018 21:34 NDZT (UTC - 13) per Priscu et al. (2021)\n",
    "slwdrill = [-278502.862779,-561384.761658]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95829fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We're going to data wrangle in this cell. \n",
    "\n",
    "# set the data paths\n",
    "\n",
    "# I load in an environment variable that tells me where I store \n",
    "# my data. That way if I need to change the path to my data \n",
    "# folder (say, point it to an external hard drive if I am in the field) \n",
    "# I can make that switch trivially\n",
    "datafold = os.getenv('DATAHOME')\n",
    "\n",
    "# quick error check to make sure $DATAHOME is set\n",
    "if datafold is None:\n",
    "    raise OSError('environment variable $DATAHOME does not exist')\n",
    "    \n",
    "# point to data locations\n",
    "\n",
    "# MOA available at: \n",
    "# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0593_moa2009_v02/geotiff/moa125_2009_hp1_v02.0.tif.gz\n",
    "# (doi: 10.5067/4ZL43A4619AF)\n",
    "# After unzipping, I then convert it to NetCDF using GMT to reduce data volume by a factor of 4 using the line:\n",
    "# > gmt grdconvert moa125_2009_hp1_v02.0.tif -Gmoa125_2009_hp1_v02.0.nc\n",
    "moa = datafold + '/MODIS/MOA/moa125_2009_hp1_v02.0.nc'\n",
    "# old MOA shapes that are good enough for continent-scale insets (and they're small)\n",
    "# grounding line: https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0593_moa2009_v02/coastlines/moa_2009_groundingline_v02.0.gmt\n",
    "# coast line: https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0593_moa2009_v02/coastlines/moa_2009_coastline_v02.0.gmt\n",
    "moa_coast = datafold + '/gz/moa/moa_2009_coastline_v02.0.gmt'\n",
    "moa_gl = datafold + '/gz/moa/moa_2009_groundingline_v02.0.gmt'\n",
    "\n",
    "# grounding line available at:\n",
    "# https://doi.pangaea.de/10013/epic.42133.d001 (doi: 10.1594/PANGAEA.819147)\n",
    "gl = datafold + '/gz/scripps/scripps_antarctica_polygons_v1.shp'\n",
    "\n",
    "# ice-surface velocity grid available at: \n",
    "# https://n5eil01u.ecs.nsidc.org/MEASURES/NSIDC-0754.001/1996.01.01/antarctic_ice_vel_phase_map_v01.nc\n",
    "# (doi: 10.5067/PZ3NJ5RXRH10)\n",
    "vel = datafold + '/velocity/measures-phase_v1/antarctic_ice_vel_phase_map_v01.nc'\n",
    "\n",
    "# drainage paths from Carter et al., 2013\n",
    "# Data directly from author\n",
    "drainage = '../data/isab_drainage.xy'\n",
    "\n",
    "# download lake outlines from github\n",
    "lake_outlines = 'SiegfriedFricker2018-outlines.h5'\n",
    "if (not os.path.exists('SiegfriedFricker2018-outlines.h5')):\n",
    "    os.system('wget -L https://github.com/mrsiegfried/Siegfried2021-GRL/raw/main/data/outlines/SiegfriedFricker2018-outlines.h5')\n",
    "lake_outlines = 'SiegfriedFricker2018-outlines.h5'\n",
    "\n",
    "# location of subglacial lake surface-height change time series\n",
    "timeseries_dir = 'data/cs2/timeseries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c37854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all of our helper functions\n",
    "\n",
    "######\n",
    "# process cryosat-2 function \n",
    "def process_cs2(cs2fold, salsa_coord_xy, dt, rez):\n",
    "    # grab files updated to present from Siegfried and Fricker(2021) processing chain\n",
    "    cs2files = glob(cs2fold + \"/*.corr\") \n",
    "    cs2files.sort() # put it in time order\n",
    "    \n",
    "    # initalize empty vectors to store output\n",
    "    times = []\n",
    "    data = []\n",
    "    n_footprints = []\n",
    "    # loop through each monthly file\n",
    "    for i in np.arange(0,len(cs2files)):\n",
    "        yr = cs2files[i][-11:-7] # grab year from file name\n",
    "        month = cs2files[i][-7:-5] # grab month from file name\n",
    "        times.append(float(yr) + (float(month)-0.5)/12) # store time of this file\n",
    "        \n",
    "        # read cryosat-2 .corr file, which has 3 months of data per file\n",
    "        df=pd.read_csv(cs2files[i],header=None, skiprows=1, delimiter=\"\\t\", \n",
    "                       usecols=[0,1,2,3,4,6], names=[\"x\",\"y\",\"dh\",\"time\",\"sigma0\",\"h_dem\"])\n",
    "        df[\"h\"] = df[\"dh\"] + df[\"h_dem\"] # convert back to elevation (from a reference height and a height change)\n",
    "        df.drop([\"dh\",\"h_dem\"], axis = 1, inplace = True) # drop the columns we don't need anymore\n",
    "        data.append(df) # add to output data\n",
    "        n_footprints.append(len(df['h'])) # save number of footprints\n",
    "        \n",
    "    # loop through each month and make a surface from the data, then interpolate DEM to the requested coordinate\n",
    "    h_cs2=np.empty((len(times),1)) # output vector for height at coordinate from CryoSat-2\n",
    "    df_salsa = pd.DataFrame(data = {'x': [salsa_coord_xy[0]], 'y': [salsa_coord_xy[1]]})\n",
    "    reg=pygmt.info(data[-1][['x','y']], spacing=1000) # get region using pygmt\n",
    "    rez=500 # resolution of DEM in m\n",
    "\n",
    "    # do the looping\n",
    "    for i in np.arange(0,len(times)):\n",
    "        if np.mod(i,10)==0:\n",
    "            print(str(i) + \" of \" + str(len(times)))\n",
    "        thisdata=data[i] # grab the right dataframe\n",
    "        thistime=times[i] # grab the right time\n",
    "\n",
    "        # make a surface in the typical way with PyGMT\n",
    "        blkmean = pygmt.blockmean(x = thisdata['x'], y = thisdata['y'], z = thisdata['h'], \n",
    "                                  spacing = rez, region = reg)\n",
    "        tmpsurf = pygmt.surface(x = blkmean[0], y = blkmean[1], z = blkmean[2], \n",
    "                                spacing = rez, region = reg, T = 0.7)\n",
    "        tmpsurffilt = pygmt.grdfilter(grid = tmpsurf, filter = \"g5000\", distance = \"0\")\n",
    "        tmpdf = pygmt.grdtrack(points = df_salsa, grid = tmpsurffilt, newcolname='zhat')\n",
    "\n",
    "        h_cs2[i] = tmpdf['zhat'][0] # save the interpolated elevation at the coordinate\n",
    "        \n",
    "    return times, h_cs2, n_footprints # return the stuff we care about\n",
    "######\n",
    "\n",
    "######\n",
    "# process icesat function: takes two tracks and determines height at a specific crossover coordinate\n",
    "def process_is(is_fold, trk1, trk2, salsa_coord_xy):\n",
    "    files1288 = glob(is_fold + '/*_' + str(trk1) + '_*.h5') # files for each campaign of track 1\n",
    "    files0369 = glob(is_fold + '/*_' + str(trk2) + '_*.h5') # files for each campaign of track 2\n",
    "    \n",
    "    # load all ICESat data from each track separate pandas dataframes. \n",
    "    # Only take data within a 250 m x 250 m box centered on coordiante provides (salsa_coord_xy). \n",
    "    dx = 250\n",
    "    dfs = []; # empty vector to store dataframes\n",
    "    for f in files1288:\n",
    "        with h5py.File(f, 'r') as fi:\n",
    "            is_x = fi['/x'][0,:]\n",
    "            is_y = fi['/y'][0,:]\n",
    "            is_z = fi['/elev'][0,:]\n",
    "            is_gain = fi['/gain'][0,:]\n",
    "            is_t = fi['/t_fracyr'][0,:]\n",
    "            camp = fi.attrs['campaign'].decode('UTF-8')\n",
    "            trk = fi.attrs['track'][0]\n",
    "        idx = (is_x<salsa_coord_xy[0]+dx) & (is_x > salsa_coord_xy[0]-dx) & (is_y<salsa_coord_xy[1]+dx) & (is_y > salsa_coord_xy[1]-dx)\n",
    "        if sum(idx) >= 0:\n",
    "            dfs.append(pd.DataFrame({'x': is_x[idx], \n",
    "                                     'y': is_y[idx], \n",
    "                                     'h': is_z[idx], \n",
    "                                     't_fracyr': is_t[idx], \n",
    "                                     'gain': is_gain[idx],\n",
    "                                     'camp': [camp] * len(is_x[idx]),\n",
    "                                     'track': [trk] * len(is_x[idx])}))\n",
    "    t1288 = pd.concat(dfs) # merge into one dataframe for the track\n",
    "\n",
    "    dfs = []; # empty vector to store dataframes\n",
    "    for f in files0369:\n",
    "        with h5py.File(f, 'r') as fi:\n",
    "            is_x = fi['/x'][0,:]\n",
    "            is_y = fi['/y'][0,:]\n",
    "            is_z = fi['/elev'][0,:]\n",
    "            is_gain = fi['/gain'][0,:]\n",
    "            is_t = fi['/t_fracyr'][0,:]\n",
    "            camp = fi.attrs['campaign'].decode('UTF-8')\n",
    "            trk = fi.attrs['track'][0]\n",
    "        idx = (is_x<salsa_coord_xy[0]+dx) & (is_x > salsa_coord_xy[0]-dx) & (is_y<salsa_coord_xy[1]+dx) & (is_y > salsa_coord_xy[1]-dx)\n",
    "        if sum(idx) >= 0:\n",
    "            dfs.append(pd.DataFrame({'x': is_x[idx], \n",
    "                                     'y': is_y[idx], \n",
    "                                     'h': is_z[idx], \n",
    "                                     't_fracyr': is_t[idx], \n",
    "                                     'gain': is_gain[idx],\n",
    "                                     'camp': [camp] * len(is_x[idx]),\n",
    "                                     'track': [trk] * len(is_x[idx])}))\n",
    "    t0369 = pd.concat(dfs)# merge into one dataframe for the track\n",
    "    \n",
    "    # make a single dataframe with all the data\n",
    "    isdata = pd.concat([t0369,t1288])\n",
    "    print(\"total points: \" + str(len(isdata['x'])))\n",
    "    \n",
    "    # solve a linear system based on Smith et al., 2009 -- this is needed to move the height time series to the drill site\n",
    "    xs = isdata['x']\n",
    "    xbar = np.mean(xs)\n",
    "    ys = isdata['y']\n",
    "    ybar = np.mean(ys)\n",
    "    zs = isdata['h']\n",
    "    zbar = np.mean(zs)\n",
    "    ts = isdata['t_fracyr']\n",
    "    tbar = np.mean(ts)\n",
    "\n",
    "    G = np.ones((len(xs),4))\n",
    "    G[:,0] = xs-xbar\n",
    "    G[:,1] = ys-ybar\n",
    "    G[:,2] = ts - tbar\n",
    "\n",
    "    coeffs = np.linalg.lstsq(G,zs, rcond=None)[0]\n",
    "    isdata['resid'] = zs - np.matmul(G,coeffs)\n",
    "\n",
    "    # solve the linear model at the coordinate we care about\n",
    "    isdata['z_drillsite'] = coeffs[0]*(salsa_coord_xy[0]-xbar) + coeffs[1]*(salsa_coord_xy[1]-ybar) + coeffs[2]*(ts-tbar) + coeffs[3] + isdata['resid']\n",
    "\n",
    "    # do some fancy pandas work to average the data in the right spots\n",
    "    isdata_grouped = isdata.groupby(['camp','track']).agg({'z_drillsite': ['mean','std'], 't_fracyr': ['mean']})\n",
    "    isdata_grouped = isdata_grouped.sort_values(('t_fracyr','mean'))\n",
    "    is_t = isdata_grouped['t_fracyr']['mean'].values\n",
    "    is_h = isdata_grouped['z_drillsite']['mean'].values\n",
    "    is_h_std = isdata_grouped['z_drillsite']['std'].values\n",
    "    \n",
    "    return is_t,is_h,is_h_std\n",
    "######\n",
    "\n",
    "######\n",
    "# function to process ICESat-2 data\n",
    "def process_is2(is2_file, times, salsa_coord_xy, dt, rez):\n",
    "    is2data=pd.read_pickle(is2file) # read the is2 data from a pre-made pickle file for speed\n",
    "    reg=pygmt.info(is2data[['x','y']], spacing=1000) # get GMT region\n",
    "    df_salsa = pd.DataFrame(data = {'x': [salsa_coord_xy[0]], 'y': [salsa_coord_xy[1]]}) \n",
    "    h_is2 = np.empty((len(times),1)) # initialize the output vector\n",
    "    n_is2 = np.empty((len(times),1)) # initialize the number of datapoints output vector\n",
    "\n",
    "    # loop through each time step, make a surface from 3 months of data centered on that time step, interpolate to coordinate\n",
    "    for i in np.arange(0,len(times)):\n",
    "        thistime=times[i] # grab time\n",
    "        idx = is2data['t_fracyr'].between(times[i]-dt/2,times[i]+dt/2) # find data in the time window centered on this time step\n",
    "        if sum(idx)>0: # only do this if there is more than 0 data\n",
    "            # print something sometimes so we know this is working\n",
    "            if thistime-np.floor(thistime)<0.1:\n",
    "                print(str(np.floor(thistime))) \n",
    "            # make a surface in the typical pyGMT way\n",
    "            blkmean = pygmt.blockmean(x = is2data['x'].loc[idx], \n",
    "                                      y = is2data['y'].loc[idx], \n",
    "                                      z = is2data['h'].loc[idx], \n",
    "                                      spacing = rez, region = reg)\n",
    "            tmpsurf = pygmt.surface(x = blkmean[0], y = blkmean[1], z = blkmean[2], spacing = rez, region = reg, T = 0.7)\n",
    "            tmpsurffilt = pygmt.grdfilter(grid = tmpsurf, filter = \"g1750\", distance = \"0\")\n",
    "            tmpdf = pygmt.grdtrack(points = df_salsa, grid = tmpsurffilt, newcolname='zhat') # interpolate to coordinate\n",
    "            h_is2[i] = tmpdf['zhat'][0] # save interpolated elevation\n",
    "            n_is2[i] = len(is2data['x'].loc[idx]) # save number of footprints in the DEM\n",
    "        h_is2[h_is2<10]=np.nan # if we have < 10 footprints, throw the data out\n",
    "    \n",
    "    return h_is2, n_is2\n",
    "######\n",
    "\n",
    "######\n",
    "# load and decimate GPS data\n",
    "def gps_make_daily(fold, station):\n",
    "    # grab all the full resolution GPS position data saved as annual files\n",
    "    gps_files = glob(fold + '/' + station + '*.xyzt') \n",
    "    dfs = [] # vector to save dataframes\n",
    "    # read each annual file into a dataframe, and store the dataframe\n",
    "    for f in gps_files:\n",
    "        print(f)\n",
    "        df = pd.read_csv(f, delimiter='\\t', header=None, names=['x','y','z','year','frac_doy'])\n",
    "        dfs.append(df)\n",
    "    pos = pd.concat(dfs) # combine all dataframes for a massive GPS dataframe for the station\n",
    "\n",
    "    # use fancy pandas math to calculate daily positions with statistics, as well as convert time to decimal years\n",
    "    pos['doy'] = np.floor(pos['frac_doy'])\n",
    "    daily = pos.groupby(['year','doy'], as_index = False).agg({'x': 'median', 'y': 'median', 'z': ['median','std','count'], 'year': 'median'})\n",
    "    # replace height values with only a few epochs with nan\n",
    "    daily.loc[daily[('z','count')] < 2*60, ('z','median')] = np.nan\n",
    "    daily['ndays'] = [365] * len(daily[('z','median')])\n",
    "    daily.loc[np.mod(daily[('year','median')],4) == 0,'ndays']=366\n",
    "    daily['fracyr'] = (daily['doy']-1)/daily['ndays'] + daily['year']['median']\n",
    "    return daily\n",
    "######\n",
    "\n",
    "######\n",
    "# load GNSS-IR results from each station into a big ol' pandas dataframe\n",
    "def gps_load_reflector_height(fold, station):\n",
    "    result = pd.DataFrame()\n",
    "    colnames=['year', 'doy', 'RH', 'Sat','UTCtime', 'Azim', 'Amp',  'eminO', 'emaxO','NumbOf','Freq','rise','EdotF','PkNoise','DelT','MJD','refr-appl'] \n",
    "    years = [f.path[-4:] for f in os.scandir(fold) if (f.is_dir()) & (f.path[-4] == '2')]\n",
    "    for year in years:\n",
    "        print(\"loading \" + station + \" data from \" + year + \".\")\n",
    "        directory = fold +'/' + year + '/results/' + station\n",
    "        files = glob(directory + \"/*.txt\")\n",
    "        for f in files:\n",
    "            df1 = pd.read_csv(f, header = 4, delim_whitespace = True, names=colnames, dtype=np.float64)\n",
    "            statname = [station]*len(df1.year)\n",
    "            df1['StationName'] = statname\n",
    "            merge = [result,df1]\n",
    "            result = pd.concat(merge)\n",
    "            \n",
    "    # do some fancy pandas math to massage data with statistics we want to keep\n",
    "    stats = result.groupby(['doy','year','StationName'], as_index = False).agg({'year': 'median', 'RH': ['median', 'count', 'std']})\n",
    "    stats['ndays'] = [365] * len(stats[('RH','median')])\n",
    "    stats.loc[np.mod(stats[('year','median')],4) == 0,'ndays'] = 366\n",
    "    stats['fracyr'] = (stats['doy']-1)/stats['ndays'] + stats[('year','median')]\n",
    "    \n",
    "    return stats\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ee8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ICESat, CryoSat-2, and ICESat-2 data\n",
    "dt = 3/12 # moving 3 month window for CryoSat-2 and ICESat-2 processing\n",
    "\n",
    "# process ICESat data\n",
    "is_fold = '../data/is'\n",
    "track1 = '1288'\n",
    "track2 = '369'\n",
    "is_t, is_h, is_h_std = process_is(is_fold, track1, track2, [drillx, drilly])\n",
    "print('done processing icesat data')\n",
    "\n",
    "# process CryoSat-2 data\n",
    "cs2fold = \"../data/cs2/slm_corr\"\n",
    "rez=500\n",
    "times, h_cs2, h_footprints = process_cs2(cs2fold, [drillx,drilly], dt, rez)\n",
    "print('done processing cryosat-2 data')\n",
    "\n",
    "# process ICESat-2\n",
    "is2file = \"../data/is2/MercerSubglacialLake.is2.atl06.004.pkl\"\n",
    "rez = 250\n",
    "h_is2, n_is2 = process_is2(is2file, times, [drillx, drilly], dt, rez)\n",
    "print ('done processing icesat-2 data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and decimate GPS data for three stations\n",
    "\n",
    "gpsfold = '../data/gps/positions'\n",
    "la09_daily = gps_make_daily(gpsfold, 'la09')\n",
    "la12_daily = gps_make_daily(gpsfold, 'la12')\n",
    "la17_daily = gps_make_daily(gpsfold, 'la17')\n",
    "print('positions loaded')\n",
    "\n",
    "gnssir_fold = '../data/gps/gnssir'\n",
    "la09ir = gps_load_reflector_height(gnssir_fold, 'la09')\n",
    "la12ir = gps_load_reflector_height(gnssir_fold, 'la12')\n",
    "la17ir = gps_load_reflector_height(gnssir_fold, 'la17')\n",
    "print('reflector heights loaded')\n",
    "\n",
    "#merge raw gps with reflector heights so we can get a snow surface time series\n",
    "la09 = pd.merge(la09_daily,la09ir, how = 'left', on = ('doy',('year','median'), 'ndays', 'fracyr'))\n",
    "la12 = pd.merge(la12_daily,la12ir, how = 'left', on = ('doy',('year','median'), 'ndays', 'fracyr'))\n",
    "la17 = pd.merge(la17_daily,la17ir, how = 'left', on = ('doy',('year','median'), 'ndays', 'fracyr'))\n",
    "print('gps data merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subglacial lakes ad drainage paths\n",
    "\n",
    "# open lake outlines hdf5 file\n",
    "h5f = h5py.File(lake_outlines, 'r')\n",
    "\n",
    "outline_geometries = [] # store polygons\n",
    "# look through each lake and load all of it's info\n",
    "for lake in h5f.keys():\n",
    "    outline_x = h5f[lake]['x'][:]\n",
    "    outline_y = h5f[lake]['y'][:]\n",
    "    outlines_xy = np.stack((outline_x, outline_y),axis=2).reshape(outline_x.shape[1], 2)\n",
    "    \n",
    "    # A single lake with multiple polygons is NaN broken---need to identify and\n",
    "    # load as a MultiPolygon. Otherwise it's easy (just load as polygon)\n",
    "    if np.isnan(outlines_xy)[:,0].sum() == 0:\n",
    "        geometry = Polygon(outlines_xy)\n",
    "    else:\n",
    "        # break at NaN values and load each as separate polygons\n",
    "        idx = np.where(np.isnan(outlines_xy[:,0]))[0]\n",
    "        \n",
    "        # grab outline of first lake before getting into the loop\n",
    "        this_outline = outlines_xy[0:idx[0],:]\n",
    "        pgons = [Polygon(this_outline)] # put the first polygon in a list\n",
    "        for i in np.arange(0,len(idx)):\n",
    "            if i == len(idx)-1:\n",
    "                this_outline = outlines_xy[idx[i]+1:,:]\n",
    "            else:\n",
    "                this_outline = outlines_xy[idx[i]+1:idx[i+1]]\n",
    "            pgons.append(Polygon(this_outline))\n",
    "        geometry = MultiPolygon(pgons)\n",
    "        \n",
    "    # append all the results in the right place\n",
    "    outline_geometries.append(geometry)\n",
    "\n",
    "# make a pandas dataframe with all the necessary info\n",
    "df = pd.DataFrame(zip(h5f.keys(), outline_geometries), \n",
    "                  columns=['name', 'geometry'])\n",
    "gdf = gpd.GeoDataFrame(df, crs=CRS(\"EPSG:3031\"), geometry=outline_geometries)\n",
    "h5f.close()\n",
    "\n",
    "### load GZ\n",
    "df_gl = gpd.read_file(gl)\n",
    "\n",
    "\n",
    "### load drainage paths\n",
    "df_drainage = pd.read_csv(drainage, \n",
    "                          header = None, \n",
    "                          names = ['x','y'], \n",
    "                          delimiter = ' ')\n",
    "#split the data frame at nans\n",
    "segs = np.split(df_drainage, np.where(np.isnan(df_drainage.x))[0])\n",
    "# removing NaN entries\n",
    "segs = [seg[~np.isnan(seg.x)] for seg in segs if not isinstance(seg, np.ndarray)]\n",
    "# removing empty DataFrames\n",
    "segs = [seg for seg in segs if not seg.empty]\n",
    "\n",
    "# some fancy geopandas work to get the drainage paths into geopandas, then crop at grounding line\n",
    "geoms = []\n",
    "for s in segs:\n",
    "    s['coords'] = s.apply(lambda row: (row['x'], row['y']), axis=1)\n",
    "    geometry = LineString(s['coords'].to_list())\n",
    "    geoms.append(geometry)\n",
    "drainpath = gpd.GeoSeries(MultiLineString(geoms))\n",
    "drainpath = gpd.GeoDataFrame(drainpath, geometry = drainpath, crs = CRS(\"EPSG:3031\"))\n",
    "\n",
    "drainpath = gpd.overlay(drainpath, gdf, how = 'difference')\n",
    "drainpath = gpd.overlay(drainpath, df_gl.loc[121:121], how = 'difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74070ec0",
   "metadata": {},
   "source": [
    "## Calculate the scaling ratio between LA09 and the drill site \n",
    "\n",
    "Because all the data are loaded, let's determine dH at the drill site between low stand and high stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CALCULATE THE EFFETIVE HEIGHT CHANGE FROM LOW STAND AT DRILL SITE\n",
    "la12['h_surf'] = la12[('z','median')] - la12[('RH','median')]\n",
    "la17['h_surf'] = la17[('z','median')] - la17[('RH','median')]\n",
    "la09['h_surf'] = la09[('z','median')] - la09[('RH','median')]\n",
    "\n",
    "\n",
    "mkratio1 = pd.merge(la12,la09, how = 'left', on = ['doy',('year','median')])\n",
    "Zx1, Zx2 = np.meshgrid(mkratio1[('h_surf_x')], mkratio1[('h_surf_x')])\n",
    "dZx = np.tril(Zx1) - np.tril(Zx2) # matrix is symmetric---just use lower triangle\n",
    "Zy1, Zy2 = np.meshgrid(mkratio1[('h_surf_y')], mkratio1[('h_surf_y')])\n",
    "dZy = np.tril(Zy1) - np.tril(Zy2) # matrix is symmetric---just use lower triangle\n",
    "\n",
    "dZx[dZx<11] = np.nan\n",
    "dZy[dZy<11] = np.nan\n",
    "dz_ratio1 = dZx.flatten()/dZy.flatten()\n",
    "\n",
    "mkratio1 = pd.merge(la17,la09, how = 'left', on = ['doy',('year','median')])\n",
    "Zx1, Zx2 = np.meshgrid(mkratio1[('h_surf_x')], mkratio1[('h_surf_x')])\n",
    "dZx = np.tril(Zx1) - np.tril(Zx2) # matrix is symmetric---just use lower triangle\n",
    "Zy1, Zy2 = np.meshgrid(mkratio1[('h_surf_y')], mkratio1[('h_surf_y')])\n",
    "dZy = np.tril(Zy1) - np.tril(Zy2) # matrix is symmetric---just use lower triangle\n",
    "\n",
    "dZx[dZx<11] = np.nan\n",
    "dZy[dZy<11] = np.nan\n",
    "dz_ratio2 = dZx.flatten()/dZy.flatten()\n",
    "\n",
    "dz_ratio = np.concatenate((dz_ratio1,dz_ratio2))\n",
    "print(\"ratio mean +/- sigma: \" + str(np.nanmean(dz_ratio)) + \" +/- \" + str(np.nanstd(dz_ratio)))\n",
    "\n",
    "h_access = la09.h_surf[(la09_daily[('year','median')] == 2018) & (la09_daily.doy == 360)]\n",
    "h_lowstand = la09[('h_surf')].min()\n",
    "dH_la09 = np.round(h_access - h_lowstand, decimals = 2)\n",
    "\n",
    "dH_drillsite = dH_la09*np.round(np.nanmean(dz_ratio), decimals=2)\n",
    "dH_drillsite_std = np.nanstd(dz_ratio)*dH_la09.iloc[0]\n",
    "print(\"dH at drill site is: \" + str(dH_drillsite.iloc[0]) + ' +/- ' + str(dH_drillsite_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d273526",
   "metadata": {},
   "source": [
    "## Finally, let's start actually plotting the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "figwidth=120 # in mm\n",
    "\n",
    "# Region for main part of the figure\n",
    "\n",
    "#   calculate centroid of SLM\n",
    "slm_poly = gdf.loc[gdf['name'] == 'MercerSubglacialLake','geometry']\n",
    "center_x = slm_poly.centroid.x.to_list()\n",
    "center_y = slm_poly.centroid.y.to_list()\n",
    "wid_km = 200\n",
    "hgt_km = 100\n",
    "slm_xl=center_x[0] - wid_km*1000/4*2\n",
    "slm_xh=center_x[0] + wid_km*1000/4*2\n",
    "slm_yl=center_y[0] - hgt_km*1000/3*2 - 5000\n",
    "slm_yh=center_y[0] + hgt_km*1000/3*1 - 5000\n",
    "\n",
    "slmratio = (slm_xh - slm_xl) / (figwidth/1000)\n",
    "figheight=figwidth*(slm_yh-slm_yl)/(slm_xh-slm_xl)\n",
    "\n",
    "slmreg = str(slm_xl) + '/' + str(slm_xh) + '/' + str(slm_yl) + '/' + str(slm_yh)\n",
    "slmproj = \"x1:\" + str(slmratio)\n",
    "slmproj_ll = \"s0/-90/-71/1:\" + str(slmratio)\n",
    "\n",
    "pygmt.makecpt(series = '15000/17000/1', cmap = 'gray', continuous = True, output = 'moa.cpt')\n",
    "with pygmt.config(COLOR_FOREGROUND = '240/249/33', COLOR_BACKGROUND = '13/8/135'):\n",
    "    pygmt.makecpt(series = [0,500,1], cmap = 'plasma', output = 'vel.cpt')\n",
    "#pygmt.makecpt(series = '-5/20/1', cmap = 'viridis', output = 'dz.cpt')\n",
    "fig = pygmt.Figure()\n",
    "with pygmt.config(MAP_FRAME_TYPE = 'inside'):\n",
    "    fig.grdimage(region = slmreg, projection = slmproj, frame=[\"nwse\", \"xf25000\", \"yf25000\"], \n",
    "                 grid = moa, cmap = 'moa.cpt')\n",
    "\n",
    "if (not quickplot):\n",
    "    veldata = xr.open_dataset(vel)\n",
    "    vel_mag = (veldata['VX']**2 + veldata['VY']**2)**0.5\n",
    "    fig.grdimage(grid = vel_mag, cmap = 'vel.cpt', transparency = 80)\n",
    "\n",
    "    # Plot graticules overtop, at 2d latitude and 15d longitude\n",
    "with pygmt.config(MAP_ANNOT_OFFSET_PRIMARY = '-2p', MAP_FRAME_TYPE = 'inside',\n",
    "                  MAP_ANNOT_OBLIQUE = 0, FONT_ANNOT_PRIMARY = '6p,grey',\n",
    "                  MAP_GRID_PEN_PRIMARY = 'grey', MAP_TICK_LENGTH_PRIMARY = '-10p',\n",
    "                  MAP_TICK_PEN_PRIMARY = 'thinner,grey', FORMAT_GEO_MAP = 'dddF',\n",
    "                  MAP_POLAR_CAP = '90/90'):\n",
    "    fig.basemap(projection = slmproj_ll, region = slmreg,\n",
    "                frame = [\"SnWE\", \"xa10g10\", \"ya1g1\"])\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not quickplot:\n",
    "    fig.plot(region = slmreg, projection = slmproj, \n",
    "             data = drainpath['geometry'], pen = \"1p,cyan\", transparency = 50)\n",
    "    fig.plot(data = gdf, \n",
    "             pen = \"0.5p,black\", \n",
    "             color = 'cyan', \n",
    "             transparency = 60)\n",
    "    fig.plot(data = gl, pen = '0.5p,white')   \n",
    "\n",
    "# plot drill sites\n",
    "fig.plot(region = slmreg, projection = slmproj, \n",
    "         x = drillx, y = drilly, style = 'a0.3c', pen='0.5p,black', color = 'red')\n",
    "fig.plot(x = slwdrill[0], y = slwdrill[1], style = 'a0.3c', pen='0.5p,black', color = 'white')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc799a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "## Plot scale bar in lower left\n",
    "wid = 20000 # scale bar width, in m\n",
    "xoff = 4000 # distance from left to put scale bar, in m\n",
    "yoff = 4000 # distance from bottom to put scale bar, in m\n",
    "\n",
    "xleft = xoff\n",
    "xmid = xoff + wid/2\n",
    "xright = xoff + wid\n",
    "\n",
    "fig.plot(x = [slm_xl + xleft, slm_xl + xright], y = [slm_yl + yoff, slm_yl + yoff], pen = '1.5p,white')\n",
    "fig.text(x = slm_xl + xmid, y = slm_yl + yoff, text = '{:3.0f}'.format(wid/1000) + ' km', \n",
    "         font = '8p,Helvetica-Bold,white', justify = 'BC', offset = '0/0.1c')\n",
    "\n",
    "# plot letter in upper right (9 pt helvetica)\n",
    "fig.text(x = slm_xl, y = slm_yh, text = 'A.', \n",
    "         font = '9p,Helvetica-Bold,white', justify = 'TL', offset = '0.1c/-0.1c')\n",
    "\n",
    "!echo -204000 -514000 MC Ross > tmptxt\n",
    "!echo -204000 -519000 MC Ice >> tmptxt\n",
    "!echo -204000 -524000 MC Shelf >> tmptxt\n",
    "!echo -308000 -520000 TL SLC >> tmptxt\n",
    "!echo -367000 -540000 TL USLC >> tmptxt\n",
    "!echo -286000 -493000 MR SLM >> tmptxt\n",
    "!echo -282000 -554000 MR SLW >> tmptxt\n",
    "fig.text(textfiles = 'tmptxt', font = '8p,Helvetica-Bold,white', justify = True)\n",
    "os.remove('tmptxt')\n",
    "\n",
    "!echo -336000 -540000 MC Whillans > tmptxt\n",
    "!echo -333500 -545000 MC Ice Stream >> tmptxt\n",
    "fig.text(textfiles = 'tmptxt', font = '8p,Helvetica-Bold,white', justify = True, angle = 20)\n",
    "os.remove('tmptxt')\n",
    "#######\n",
    "\n",
    "!echo -312000 -482000 MC Mercer > tmptxt\n",
    "!echo -316000 -485500 MC Ice Stream >> tmptxt\n",
    "fig.text(textfiles = 'tmptxt', font = '8p,Helvetica-Bold,white', justify = True, angle = -50)\n",
    "os.remove('tmptxt')\n",
    "#######\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c21051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "## Plot legend with symbols; plot scale bars below the figure\n",
    "# make legend file and plot it\n",
    "!echo 'S - a 0.3c white 0.25p,black - Drill sites' > legend.txt\n",
    "!echo 'S - - 0.2c - 1p,cyan - Water flowpath' >> legend.txt\n",
    "!echo 'S - - 0.2c - 1p,white - Grounding line' >> legend.txt\n",
    "with pygmt.config(FONT_ANNOT_PRIMARY = '8p', FONT_LABEL='8p,Helvetica', \n",
    "                  MAP_FRAME_PEN = '0.5p'):\n",
    "    fig.legend(region = slmreg, projection = slmproj, \n",
    "               spec = 'legend.txt', position = 'JBR+jBR+o0.1c', \n",
    "               box = '+gwhite+p0.5p', transparency = 30)\n",
    "    fig.legend(spec = 'legend.txt', position = 'JBR+jBR+o0.1c')\n",
    "os.remove('legend.txt')\n",
    "\n",
    "# make the velocity colorbar 20% figure length, in the upper right\n",
    "barwidth = figwidth/10 * 0.2\n",
    "pos_str = 'n1/1+w' + str(barwidth) + 'c+h+ml+jTR+o0.45c/0.6c'\n",
    "with pygmt.config(FONT_ANNOT_PRIMARY = '8p', FONT_LABEL = '8p', \n",
    "                  MAP_ANNOT_OFFSET_PRIMARY = '2p', MAP_TICK_PEN_PRIMARY = '0.5p', \n",
    "                  MAP_TICK_LENGTH_PRIMARY = '3p', MAP_FRAME_PEN = '0.5p', MAP_LABEL_OFFSET = '4p'):\n",
    "    fig.colorbar(cmap = 'vel.cpt', position = pos_str, box = '+gwhite+p0.5p',\n",
    "                 frame = 'xa250f50+l\"Ice velocity (m a@+-1@+)\"', transparency = 30) \n",
    "    fig.colorbar(cmap = 'vel.cpt', position = pos_str, frame = 'xa250f50+l\"Ice velocity (m a@+-1@+)\"') \n",
    "#######\n",
    "\n",
    "\n",
    "# Make insets\n",
    "antwidth = 2.5 # width of inset in cm\n",
    "antreg = '-2900000/-2900000/2900000/2900000r'\n",
    "antmap = 'X' + str(antwidth) + 'c'\n",
    "\n",
    "# generate offset string based on inset width\n",
    "xoff_str = 'a' + str(0.05) + 'c'\n",
    "yoff_str = 'a' + str(figheight/10 - antwidth - 0.05) + 'c'\n",
    "\n",
    "fig.basemap(projection = antmap, region = antreg, frame = '+n', xshift = xoff_str, yshift = yoff_str)\n",
    "fig.plot(data = moa_coast, color = 'white', xshift = xoff_str, yshift = yoff_str)\n",
    "fig.plot(data = moa_gl, color = 'gray', xshift = xoff_str, yshift = yoff_str)\n",
    "fig.plot(x = [slm_xl, slm_xl, slm_xh, slm_xh, slm_xl], y = [slm_yl, slm_yh, slm_yh, slm_yl, slm_yl], \n",
    "         pen = '1p,black', xshift = xoff_str, yshift = yoff_str)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a01dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inset_t_min = 2018.9\n",
    "inset_t_max = 2020.1\n",
    "inset_h_min = 102\n",
    "inset_h_max = 111\n",
    "\n",
    "t_min = 2003\n",
    "t_max = 2022\n",
    "h_max = 119\n",
    "h_min = 96\n",
    "ts_reg = str(t_min) + '/' + str(t_max) + '/' + str(h_min) + '/' + str(h_max)\n",
    "wid = 6 # in cm \n",
    "hgt = wid/2\n",
    "\n",
    "x_off = 0.3 #in cm\n",
    "t_off = (t_max - t_min)/wid * x_off\n",
    "yskip = 0.75 # in cm\n",
    "\n",
    "ts_proj = 'X' + str(wid) + 'c/' + str(hgt) + 'c'\n",
    "with pygmt.config(FONT_LABEL='8p,Helvetica', FONT_ANNOT_PRIMARY = '8p,Helvetica', \n",
    "                  MAP_GRID_PEN_PRIMARY = '0.5p,black,dashed'):\n",
    "    fig.basemap(region = ts_reg, projection = ts_proj,\n",
    "                yshift = str(-hgt-0.15) + 'c', xshift = '1.1c', \n",
    "                frame=['nWSe', 'xa4f2g1', 'ya5'])\n",
    "fig.text(x = t_min, y = (h_min+h_max)/2, text = 'Surface elev. (m)',\n",
    "         font = '8p,Helvetica,black', justify = 'BC', angle = 90,\n",
    "         offset = '-0.8c/0c', no_clip = True)    \n",
    "\n",
    "####### plot the data\n",
    "cb = cmocean.tools.get_dict(cmocean.cm.matter, N=6)\n",
    "\n",
    "# plot icesat data\n",
    "is_color = str(cb['red'][1][1]*255)+'/'+str(cb['green'][1][1]*255)+'/'+str(cb['blue'][1][1]*255)\n",
    "fig.plot(x = is_t, y = is_h, pen = '1p,'+is_color+',2_2:0p')\n",
    "fig.plot(x = is_t, y = is_h, style = 't0.25c',  pen = '0.25p,black',\n",
    "         color = is_color, label = 'ICESat')\n",
    "\n",
    "# plot CryoSat-2 data\n",
    "#ax.plot(times,h_cs2,'s',color=(cb['red'][1][1], cb['green'][1][1], cb['blue'][1][1]),label='CryoSat-2', mec='black')\n",
    "cs2_color = str(cb['red'][2][1]*255)+'/'+str(cb['green'][2][1]*255)+'/'+str(cb['blue'][2][1]*255)\n",
    "fig.plot(x = times, y = h_cs2[:,0], style = 's0.15c', pen = '0.25p,black', \n",
    "         color = cs2_color, label = 'CryoSat-2')\n",
    "\n",
    "# plot gps data\n",
    "gps_color = str(cb['red'][3][1]*255)+'/'+str(cb['green'][3][1]*255)+'/'+str(cb['blue'][3][1]*255)\n",
    "fig.plot(x = la12['fracyr'], y = la12[('z','median')] - la12[('RH','median')], \n",
    "         style = 'c0.1c', color = gps_color, label = 'GPS')\n",
    "fig.plot(x = la17['fracyr'], y = la17[('z','median')] - la17[('RH','median')], \n",
    "         style = 'c0.1c', color = gps_color)\n",
    "\n",
    "\n",
    "# plot ICESat-2 data\n",
    "is2_color = str(cb['red'][4][1]*255)+'/'+str(cb['green'][4][1]*255)+'/'+str(cb['blue'][4][1]*255)\n",
    "#ax.plot(times,h_is2,color = (cb['red'][4][1], cb['green'][4][1], cb['blue'][4][1]), lw=2)\n",
    "#ax.plot(times,h_is2,'v', label='ICESat-2', markersize=10, mec='black',\n",
    "#        color = (cb['red'][4][1], cb['green'][4][1], cb['blue'][4][1]))\n",
    "fig.plot(x = times, y = h_is2[:,0], pen = '1p,'+is2_color)\n",
    "fig.plot(x = times, y = h_is2[:,0], style = 'i0.2c', pen = '0.25p,black', \n",
    "         color = is2_color, label = 'ICESat-2')\n",
    "#######\n",
    "\n",
    "with pygmt.config(FONT_ANNOT_PRIMARY='6p,Helvetica'):\n",
    "    fig.legend(transparency=10, position = 'JTL+jTL+o0.02c', box = '+gwhite+c0c/0c')\n",
    " \n",
    "\n",
    "fig.plot(x = [breakthrough,breakthrough], y = [h_min,h_max], pen = '1p,red')\n",
    "fig.text(x = [breakthrough], y = h_max, text = 'Sampling', \n",
    "         font = '6p,Helvetica,red', justify = 'TR', offset='-0.05c/-0.1c', fill='white')\n",
    "fig.plot(x = [inset_t_min, inset_t_min, inset_t_max, inset_t_max, inset_t_min], \n",
    "         y = [inset_h_min, inset_h_max, inset_h_max, inset_h_min, inset_h_min], \n",
    "         pen = '0.5p,black')\n",
    "\n",
    "# plot letter in upper left (9 pt helvetica)\n",
    "fig.text(x = t_min, y = h_max, text = 'B.', \n",
    "         font = '9p,Helvetica-Bold,black', justify = 'TL', \n",
    "         offset = '-1c/0c', no_clip = True)\n",
    "\n",
    "newproj = 'X' + str(wid + x_off) + 'c/' + str(hgt) + 'c'\n",
    "newreg = str(t_min) + '/' + str(t_max+t_off) + '/' + str(h_min) + '/' + str(h_max)\n",
    "fig.plot(region = newreg, projection = newproj,\n",
    "         x = [inset_t_max, t_max+t_off], y = [inset_h_max, h_max], \n",
    "         pen = '0.5p,black', no_clip = True)\n",
    "fig.plot(x = [inset_t_max, t_max+t_off], y = [inset_h_min, h_min+(h_max-h_min)/3], \n",
    "         pen = '0.5p,black', no_clip = True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inset_reg = str(inset_t_min) + '/' + str(inset_t_max) + '/' + str(inset_h_min) + '/' + str(inset_h_max)\n",
    "inset_wid = wid/2\n",
    "inset_hgt = hgt/3*2\n",
    "inset_proj = 'X' + str(inset_wid) + 'c/' + str(inset_hgt) + 'c'\n",
    "\n",
    "xshift = str(wid + x_off) + 'c'\n",
    "\n",
    "l0_off = '1c/0c'\n",
    "l1_off = '1.13c/0c'\n",
    "l2_off = '1.26c/0'\n",
    "\n",
    "######## GPS DURING LAKE GEOPHYSICS YEAR\n",
    "with pygmt.config(FONT_LABEL='8p,Helvetica', FONT_ANNOT_PRIMARY = '8p,Helvetica'):\n",
    "    fig.basemap(region = inset_reg, projection = inset_proj,\n",
    "                yshift = str(inset_hgt/2) + 'c', xshift = xshift, \n",
    "                frame=['nsE', 'xa1f0.25g1', 'ya5'])\n",
    "    fig.plot(x = [inset_t_min, inset_t_min], y = [inset_h_min, inset_h_max], pen = \"0.75p,black\")\n",
    "    fig.text(x = inset_t_max, y = (inset_h_min+inset_h_max)/2, \n",
    "             text = 'Surface', justify = 'TC', offset = l0_off, \n",
    "             font = '8p,Helvetica,black', no_clip = True, angle = 90)\n",
    "    fig.text(x = inset_t_max, y = (inset_h_min+inset_h_max)/2, \n",
    "             text = 'Elevation (m)', justify = 'TC', offset = l2_off, \n",
    "             font = '8p,Helvetica,black', no_clip = True, angle = 90)\n",
    "gps_color = str(cb['red'][3][1]*255)+'/'+str(cb['green'][3][1]*255)+'/'+str(cb['blue'][3][1]*255)\n",
    "fig.plot(x = la17['fracyr'], y = la17[('z','median')] - la17[('RH','median')], \n",
    "             style = 'c0.1c', color = gps_color, label = 'GPS')\n",
    "with pygmt.config(FONT_ANNOT_PRIMARY='6p,Helvetica'):\n",
    "    fig.legend(transparency=10, position = 'JBR+jBR+o0.04c', box = '+gwhite+c0c/0c' )\n",
    " \n",
    "             \n",
    "fig.plot(x = [breakthrough,breakthrough], y = [inset_h_min,inset_h_max], pen = '1p,red')\n",
    "    \n",
    "######## ApRES DURING LAKE GEOPHYSICS YEAR\n",
    "d = io.loadmat('../data/apres/RangeOverTime.mat')\n",
    "apres = pd.DataFrame(data = {'range': d['range'][:,0], 'doy': d['timeInDays'][:,0]})\n",
    "apres['t_fracyr'] = 2019 + (apres['doy'] - 1)/365\n",
    "\n",
    "thk_min = 1093.8\n",
    "thk_max = 1096.2\n",
    "inset_reg = str(inset_t_min) + '/' + str(inset_t_max) + '/' + str(thk_min) + '/' + str(thk_max)\n",
    "with pygmt.config(FONT_LABEL='8p,Helvetica', FONT_ANNOT_PRIMARY = '8p,Helvetica', \n",
    "                  MAP_GRID_PEN_PRIMARY = '0.5p,black,dashed'):\n",
    "    fig.basemap(region = inset_reg, projection = inset_proj, \n",
    "                yshift = str(-inset_hgt - yskip/3) + 'c', \n",
    "                frame=['nsE', 'xa1f0.25g1', 'ya1'])\n",
    "fig.plot(x = [inset_t_min, inset_t_min], y = [thk_min, thk_max], pen = \"0.75p,black\")\n",
    "fig.text(x = inset_t_max, y = (thk_min+thk_max)/2, \n",
    "         text = 'Ice thickness (m)', justify = 'TC', offset = l1_off, \n",
    "         font = '8p,Helvetica,black', no_clip = True, angle = 90)\n",
    "fig.plot(x = apres['t_fracyr'], y = apres['range'], \n",
    "         style = 'c0.1c', color = 'darkblue', label = 'ApRES')\n",
    "\n",
    "with pygmt.config(FONT_ANNOT_PRIMARY='6p,Helvetica'):\n",
    "    fig.legend(transparency=10, position = 'JBR+jBR+o0.04c', box = '+gwhite+c0c/0c' )\n",
    " \n",
    "    fig.plot(x = [breakthrough,breakthrough], y = [thk_min,thk_max], pen = '1p,red')\n",
    "    \n",
    "    # plot letter in upper right (9 pt helvetica)\n",
    "fig.text(x = inset_t_min, y = thk_max, text = 'D.', \n",
    "         font = '9p,Helvetica-Bold,black', justify = 'TL', offset = '0.1c/-0.1c', \n",
    "         transparency = 10, fill = 'white')\n",
    "    \n",
    "######## laser attenuation DURING LAKE GEOPHYSICS YEAR\n",
    "la = pd.read_csv('../data/dts/salsa_slm_dts_atten_data.csv')\n",
    "# convert year-month-day-hour-min-sec to fractional years\n",
    "date = np.column_stack((la['Year'], la['Month'], la['Day'], la['Hour'], la['Minute'], la['Minute']*0))\n",
    "dts_times = [datetime.datetime(y,m,d,h,mi,s) for y,m,d,h,mi,s in date]\n",
    "la['fracyr'] = [(float(dts_t.strftime(\"%j\"))-1) / 366 + float(dts_t.strftime(\"%Y\")) for dts_t in dts_times]\n",
    "# make the confidence interval vectors\n",
    "dts_int95_t = np.concatenate((la['fracyr'].to_numpy(), np.flip(la['fracyr'].to_numpy())),axis=0)\n",
    "dts_int95_d = np.concatenate((la['95 CI Lower Bound']*1000, np.flip(la['95 CI Upper Bound']*1000)))\n",
    "\n",
    "att_min = -.2\n",
    "att_max = .3 \n",
    "inset_reg = str(inset_t_min) + '/' + str(inset_t_max) + '/' + str(att_min) + '/' + str(att_max)\n",
    "with pygmt.config(FONT_LABEL='8p,Helvetica', FONT_ANNOT_PRIMARY = '8p,Helvetica', \n",
    "                  MAP_GRID_PEN_PRIMARY = '0.5p,black,dashed'):\n",
    "    fig.basemap(region = inset_reg, projection = inset_proj, \n",
    "                yshift = str(-inset_hgt - yskip/3) + 'c', \n",
    "                frame=['nSE', 'xa1f0.25g1+l\"Year\"', 'ya0.1'])\n",
    "    fig.plot(x = [inset_t_min, inset_t_min], y = [att_min, att_max], pen = \"0.75p,black\")\n",
    "    fig.text(x = inset_t_max, y = (att_min+att_max)/2, \n",
    "             text = 'Differential atten.', justify = 'TC', offset = l0_off, \n",
    "             font = '8p,Helvetica,black', no_clip = True, angle = 90)\n",
    "    fig.text(x = inset_t_max, y = (att_min+att_max)/2, \n",
    "             text = 'rate (km@+-1@+)', justify = 'TC', offset = l2_off, \n",
    "             font = '8p,Helvetica,black', no_clip = True, angle = 90)\n",
    "    \n",
    "    fig.plot(x = dts_int95_t, y= dts_int95_d, color = 'black', transparency = 80)\n",
    "    fig.plot(x = la['fracyr'], y = la['Inferred Da (m-1)']*1000, pen = \"0.5p,black\",\n",
    "             label = 'FOS')\n",
    "with pygmt.config(FONT_ANNOT_PRIMARY='6p,Helvetica'):\n",
    "    fig.legend(transparency=10, position = 'JBR+jBR+o0.04c', box = '+gwhite+c0c/0c' )\n",
    "fig.plot(x = [breakthrough,breakthrough], y = [att_min,att_max], pen = '1p,red')\n",
    "\n",
    "# plot letter in upper right (9 pt helvetica)\n",
    "fig.text(x = inset_t_min, y = att_max, text = 'E.', \n",
    "         font = '9p,Helvetica-Bold,black', justify = 'TL', offset = '0.1c/-0.1c', \n",
    "         transparency = 10, fill = 'white')\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs2slm = pd.read_csv('../data/cs2/timeseries/slm-20210721.dat', header = None, names = ['year','dh'])\n",
    "cs2slc = pd.read_csv('../data/cs2/timeseries/slc-20210721.dat', header = None, names = ['year','dh'])\n",
    "cs2uslc = pd.read_csv('../data/cs2/timeseries/uslc-20210721.dat', header = None, names = ['year','dh'])\n",
    "area_slm = 143\n",
    "area_slc = 267\n",
    "area_uslc = 187\n",
    "\n",
    "h_min = -0.3\n",
    "h_max = 1.3\n",
    "\n",
    "new_t_min = 2010\n",
    "newwid = wid / (t_max - t_min) * (t_max - new_t_min)\n",
    "\n",
    "newxshift = str(newwid + x_off) + 'c'\n",
    "\n",
    "new_ts_proj = 'X' + str(newwid) + 'c/' + str(hgt) + 'c'\n",
    "ts_reg = str(new_t_min) + '/' + str(t_max) + '/' + str(h_min) + '/' + str(h_max)\n",
    "\n",
    "with pygmt.config(FONT_LABEL='8p,Helvetica', FONT_ANNOT_PRIMARY = '8p,Helvetica', \n",
    "                  MAP_GRID_PEN_PRIMARY = '0.5p,black,dashed'):\n",
    "    fig.basemap(region = ts_reg, projection = new_ts_proj,\n",
    "                xshift = '-' + newxshift,\n",
    "                frame=['nWSe', 'xa4f2g1+l\"Year\"', 'ya0.5'])\n",
    "\n",
    "fig.plot(x = cs2uslc['year'], y = (cs2uslc['dh'] - cs2uslc['dh'].min())/1000 * area_uslc - 0.2, \n",
    "         style = 'c0.15c', pen = '0.25p,black', label = 'USLC', color = 'white')\n",
    "fig.plot(x = cs2slc['year'], y = (cs2slc['dh'] - cs2slc['dh'].min())/1000 * area_slc - 0.2, \n",
    "         style = 'c0.15c', pen = '0.25p,black', label = 'SLC', color = 'gray')\n",
    "fig.plot(x = cs2slm['year'], y = (cs2slm['dh'] - cs2slm['dh'].min())/1000 * area_slm, \n",
    "         style = 's0.15c', pen = '0.25p,black', label = 'SLM', color = 'black')\n",
    "\n",
    "fig.text(x = new_t_min, y = (h_min+h_max)/2, text = 'Vol. change (km@+3@+)',\n",
    "         font = '8p,Helvetica,black', justify = 'BC', angle = 90,\n",
    "         offset = '-0.8c/0c', no_clip = True)\n",
    "    \n",
    "fig.text(x = new_t_min, y = h_max, text = 'C.', \n",
    "         font = '9p,Helvetica-Bold,black', justify = 'TL', \n",
    "         offset = '-1c/0c', no_clip = True)\n",
    "fig.plot(x = [breakthrough,breakthrough], y = [h_min,h_max], pen = '1p,red')\n",
    "\n",
    "with pygmt.config(FONT_ANNOT_PRIMARY='6p,Helvetica'):\n",
    "    fig.legend(transparency=10, position = 'JBR+jBR+o0.04c', box = '+gwhite+c0c/0c' )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ebb4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "os.remove('moa.cpt')\n",
    "os.remove('vel.cpt')\n",
    "os.remove('SiegfriedFricker2018-outlines.h5')\n",
    "\n",
    "# save\n",
    "fig.savefig(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1cdf1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siegvent2023",
   "language": "python",
   "name": "siegvent2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
